{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dial-a-ride problem using Transformers and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import DarpEnv\n",
    "from log import logger, set_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment (env.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = set_level(logger, \"debug\")\n",
    "\n",
    "# loading and initializing a darp environment\n",
    "FILE_NAME = 'data/cordeau/a2-16.txt'\n",
    "env = DarpEnv(size=10, nb_requests=16, nb_vehicles=2, time_end=1440, max_step=1000, dataset=FILE_NAME)\n",
    "obs = env.representation()\n",
    "\n",
    "# simulate env with nearest neighbor action\n",
    "rewards = []\n",
    "for t in range(100):\n",
    "    action = env.nearest_action_choice()    \n",
    "    obs, reward, done = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    delivered =  sum([request.state == \"delivered\" for request in env.requests])\n",
    "    all_delivered = env.is_all_delivered()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.penalize_broken_time_windows()\n",
    "\n",
    "# print out results\n",
    "total = sum([v.total_distance_travelled for v in env.vehicles])\n",
    "logger.info(f\"Episode finished after {t + 1} steps, with reward {total}\")\n",
    "for vehicle in env.vehicles:\n",
    "    logger.info(f'{vehicle} history: {vehicle.history}')\n",
    "delivered =  sum([request.state == \"delivered\" for request in env.requests])\n",
    "in_trunk = sum([r.state == \"in_trunk\" for r in env.requests])\n",
    "pickup = sum([r.state == \"pickup\" for r in env.requests])\n",
    "logger.info(f'delivered: {delivered}, in trunk: {in_trunk}, waiting: {pickup}')\n",
    "logger.info(f'delivered: {delivered}, in trunk: {in_trunk}, waiting: {pickup}')\n",
    "logger.info(\"*** PENALTY ***\")\n",
    "logger.info(\"start_window: %s\", env.penalty[\"start_window\"])\n",
    "logger.info(\"end_window: %s\", env.penalty[\"end_window\"])\n",
    "logger.info(\"max_route_duration: %s\", env.penalty[\"max_route_duration\"])\n",
    "logger.info(\"max_ride_time: %s\", env.penalty[\"max_ride_time\"])\n",
    "logger.info(\"total penalty: %s\", env.penalty[\"sum\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests and Vehicles (entity.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is populated with Request and Vehicle objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity import Request, Vehicle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Vehicle(id=0,\n",
    "                position=np.array([0,0]),\n",
    "                capacity=3,\n",
    "                max_route_duration=300)\n",
    "\n",
    "\n",
    "request = Request(id=0,\n",
    "                pickup_position=np.array([1,2]),\n",
    "                dropoff_position=np.array([3,4]),\n",
    "                service_time = 3,\n",
    "                start_window=np.array([0,100]),\n",
    "                end_window=np.array([100,200]),\n",
    "                max_ride_time=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating & loading datasets (generate.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating a list of DarpEnv objects, with randomly located requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import generate_environments, dump_data, load_aoyo\n",
    "\n",
    "logger = set_level(logger, \"info\")\n",
    "envs = generate_environments(N=10000,\n",
    "                        size= 10, \n",
    "                        nb_vehicles=4,\n",
    "                        nb_requests=48,\n",
    "                        time_end=1440,\n",
    "                        max_step=1000,\n",
    "                        max_route_duration=720,\n",
    "                        capacity=3,\n",
    "                        max_ride_time=30,\n",
    "                        window=True)\n",
    "\n",
    "    \n",
    "logger.info(\"data dump starts...\")\n",
    "path = \"data/processed/generated-10000-a4-48.pkl\"\n",
    "dump_data(envs, path)\n",
    "logger.info(\"data successfully dumped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Aoyu's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = \"a2-16\"\n",
    "train_envs, test_envs = load_aoyo(instance)\n",
    "\n",
    "logger.info(\"data dump starts...\")\n",
    "train_path = f\"data/processed/aoyu-10000-{instance}-train.pkl\"\n",
    "test_path = f\"data/processed/aoyu-10000-{instance}-test.pkl\"\n",
    "dump_data(train_envs, train_path)\n",
    "dump_data(test_envs, test_path)\n",
    "logger.info(\"data successfully dumped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training supervised learning model (supervised.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised import supervised_trainer\n",
    "from model import Aoyu\n",
    "import torch\n",
    "from utils import get_device\n",
    "\n",
    "instance=\"a4-48\"\n",
    "result_path = \"models\"\n",
    "supervised_policy=\"rf\"\n",
    "trial = \"01\"\n",
    "batch_size = 256\n",
    "nb_epochs = 10\n",
    "id = f\"result-{instance}-supervised-{supervised_policy}-{trial}-aoyu256\"\n",
    "\n",
    "#initialize policy\n",
    "policy = Aoyu(d_model=256, nhead=8, nb_requests=48, nb_vehicles=4, num_layers=4, time_end=1440, env_size=10)\n",
    "device = get_device()\n",
    "policy = policy.to(device)\n",
    "logger.info(\"training on device: %s\", device)\n",
    "\n",
    "#initialize optimizer\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "\n",
    "#start train\n",
    "result = supervised_trainer(id, \n",
    "                        instance,\n",
    "                        result_path,\n",
    "                        supervised_policy,\n",
    "                        batch_size, \n",
    "                        nb_epochs, \n",
    "                        policy,\n",
    "                        optimizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a reinforcement learning model (model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import seed_everything\n",
    "from generator import load_data\n",
    "from model import reinforce_trainer\n",
    "\n",
    "seed_everything(1)\n",
    "logger = set_level(logger, \"info\")\n",
    "\n",
    "result_path = \"models\"\n",
    "nb_vehicles = 2\n",
    "nb_requests = 16\n",
    "variant = \"a\"\n",
    "instance = f\"{variant}{nb_requests}-{nb_vehicles}\"\n",
    "test_env_path = f'data/cordeau/{instance}.txt'  \n",
    "id = f\"result-{instance}-reinforce-01-aoyu\"\n",
    "\n",
    "nb_episodes= 1000\n",
    "update_baseline = 100\n",
    "\n",
    "# load model from supervised training\n",
    "policy = Aoyu(d_model=256, nhead=8, nb_requests=nb_requests, nb_vehicles=nb_vehicles, num_layers=4, time_end=1440, env_size=10)\n",
    "PATH = \"models/result-a2-16-supervised-rf-01-aoyu256\"\n",
    "r = load_data(PATH)\n",
    "state = r.policy_dict\n",
    "policy.load_state_dict(state)\n",
    "\n",
    "# pass model to CUDA if available\n",
    "device = get_device()\n",
    "policy = policy.to(device)\n",
    "logger.info(\"training on device: %s\", device)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "reinforce_trainer(test_env_path, result_path, id ,nb_episodes, nb_requests, nb_vehicles, update_baseline, policy, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test set (evaluate.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on one instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_model, evaluate_aoyu\n",
    "\n",
    "logger = set_level(logger, \"info\")\n",
    "\n",
    "# loading the darp instance\n",
    "FILE_NAME = 'data/cordeau/a2-16.txt'\n",
    "test_env = DarpEnv(size=10, nb_requests=16, nb_vehicles=2, time_end=1440, max_step=34, dataset=FILE_NAME)\n",
    "\n",
    "policy = Aoyu(d_model=256, nhead=8, nb_requests=16, nb_vehicles=2, num_layers=4, time_end=1440, env_size=10)\n",
    "# loading a Result object, containing a state_dict of a trained model (WARNING: for now model hyperparameters are not stored in the result object) \n",
    "PATH = \"models/result-a2-16-supervised-rf-01-aoyu256\"\n",
    "r = load_data(PATH)\n",
    "state = r.policy_dict\n",
    "policy.load_state_dict(state)\n",
    "\n",
    "# passing the model to CUDA if available \n",
    "device = get_device()\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "\n",
    "routing_cost, window_penalty, delivered = evaluate_model(policy, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on 1.000 instance from Aoyu's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = set_level(logger, \"info\")\n",
    "\n",
    "instance = \"a2-16\"\n",
    "test_path = f\"data/aoyu/{instance}-test.txt\"\n",
    "\n",
    "policy = Aoyu(d_model=256, nhead=8, nb_requests=16, nb_vehicles=2, num_layers=4, time_end=1440, env_size=10)\n",
    "# loading a Result object, containing a state_dict of a trained model (WARNING: for now model hyperparameters are not stored in the result object) \n",
    "PATH = \"models/result-a2-16-supervised-rf-01-aoyu256\"\n",
    "r = load_data(PATH)\n",
    "state = r.policy_dict\n",
    "policy.load_state_dict(state)\n",
    "\n",
    "# passing the model to CUDA if available \n",
    "device = get_device()\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "\n",
    "df = evaluate_aoyu(policy, test_path)\n",
    "df.to_csv(f\"evaluations/data-{instance}-test-model-rf-a2-16-02\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27252ed1c8bf73e55b7e66ced66f5778e98f2c0da91b1c15f11858aad69ec6d9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('darp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
